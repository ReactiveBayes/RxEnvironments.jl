var documenterSearchIndex = {"docs":
[{"location":"lib/advanced_example/#lib-advanced-examples","page":"Advanced Usage","title":"Advanced Usage","text":"","category":"section"},{"location":"lib/advanced_example/#Changing-the-emission-rate-and-environment-speed","page":"Advanced Usage","title":"Changing the emission rate and environment speed","text":"","category":"section"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"By default, any RxEnvironment emits an observation to any subscribed agents every 1000 milliseconds, or whenever any agent in the environment conducts an action. To change this, one can use the emit_every_ms keyword argument to the RxEnvironment function. Taking the environment from the example:","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"environment = RxEnvironment(BayesianThermostat(0.0, -10, 10); emit_every_ms = 10)","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"Will emit an observation to any agents in the environment every 10 milliseconds.","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"By adjusting the real_time_factor keyword argument to the RxEnvironment function, one can play with the amount of computation time we give to agents to conduct their actions. For example:","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"environment = RxEnvironment(BayesianThermostat(0.0, -10, 10); emit_every_ms = 1000, real_time_factor=2)","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"Will emit an observation to every subscribed agent every 1000 milliseconds. However, the environment will only have moved 500 milliseconds forward, giving any subscribed agent twice as much time to choose an action than that it would have in a real-time setting.","category":"page"},{"location":"lib/advanced_example/#Discrete-Environments","page":"Advanced Usage","title":"Discrete Environments","text":"","category":"section"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"RxEnvironments natively represents any environment as a continuous environment. However, discrete environments are also supported. By including the keyword argument discrete=true to the RxEnvironment function, we convert the environment to a discrete environment. There are 2 major differences between a discrete RxEnvironment and a continuous one:","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"A discrete environment waits until all agents in the environment have conducted an action, and only then takes the last action emitted by every agent into account. I.e. if we have an environment with agent_1 and agent_2 as agents. If agent_1 emits two actions before agent_2 emits, the environment will only incorporate the second action emitted by agent_1 whenever agent_2 emits.\nA discrete environment needs to implement update!(::EnvironmentType), without the elapsed_time argument, since the state-transition does not depend on the elapsed time.","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"Tip: by implementing update!(::Environment) = update!(::Environment, dt) for a constant dt, a custom environment can be initialized both as a continuous as a discrete environment.","category":"page"},{"location":"lib/advanced_example/#Animating-Environments","page":"Advanced Usage","title":"Animating Environments","text":"","category":"section"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"Animating an environment is done using the GLMakie package. In order to animate your custom environments, please implement RxEnvironments.plot_state(ax, ::EnvironmentType), where ax is the GLMakie axis object that you can plot towards. If you need access to other agents or entities in order to plot your environments, you can extend RxEnvironments.add_to_state!(::EnvironmentType, ::AgentType) to make sure you have access to subscribed agents in the state.","category":"page"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"By calling RxEnvironments.animate_state(::RxEnvironment; fps), RxEnvironments animates the plots you generate in the plot_state function to accurately reflect the state of your environment in real time.","category":"page"},{"location":"lib/advanced_example/#Multi-Agent-Environments","page":"Advanced Usage","title":"Multi-Agent Environments","text":"","category":"section"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"RxEnvironments natively supports multi-agent environments, similarly to how we call add!(environment, agent) in the example page, we can call add! with additional agents in order to create a multi-agent environment. ","category":"page"},{"location":"lib/advanced_example/#Inspecting-Observations","page":"Advanced Usage","title":"Inspecting Observations","text":"","category":"section"},{"location":"lib/advanced_example/","page":"Advanced Usage","title":"Advanced Usage","text":"By using RxEnvironments.subscribe_to_observations! we can subscribe any Rocket actor to the observations of any entity. Note that these observations will be of type Observation, that also contains a reference to the entity that emitted the observation. In order to retrieve the value of the observation, you can call RxEnvironments.data on the Observation instance.","category":"page"},{"location":"lib/getting_started/#lib-started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"When using RxEnvironments, you only have to specify the dynamics of your environment. Let's create the Bayesian Thermostat environment in RxEnvironments. For this example, you need Distributions.jl installed in your environment as well. ","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"Let's create the basics of our environment:","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"using RxEnvironments\nusing Distributions\n\n# Empty agent, could contain states as well\nstruct ThermostatAgent end\n\nmutable struct BayesianThermostat\n    temperature::Real\n    min_temp::Real\n    max_temp::Real\nend\n\n# Helper functions\ntemperature(env::BayesianThermostat) = env.temperature\nmin_temp(env::BayesianThermostat) = env.min_temp\nmax_temp(env::BayesianThermostat) = env.max_temp\nnoise(env::BayesianThermostat) = Normal(0.0, 0.1)\nset_temperature!(env::BayesianThermostat, temp::Real) = env.temperature = temp\nfunction add_temperature!(env::BayesianThermostat, diff::Real) \n    env.temperature += diff\n    if temperature(env) < min_temp(env)\n        set_temperature!(env, min_temp(env))\n    elseif temperature(env) > max_temp(env)\n        set_temperature!(env, max_temp(env))\n    end\nend","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"By implementing RxEnvironments.act!, RxEnvironments.observe and RxEnvironments.update! for our environment, we can fully specify the behaviour of our environment, and RxEnvironments will take care of the rest. In order to follow along with the sanity checks, please install Rocket.jl as well.","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"RxEnvironments.act!(env::BayesianThermostat, actor::ThermostatAgent, action::Float64) = add_temperature!(env, action)\n\n# The agent receives a noisy observation of the environment's temperature \nRxEnvironments.observe(receiver::ThermostatAgent, emitter::BayesianThermostat) = temperature(emitter) + rand(noise(emitter))\n\n# The environment cools down over time.\nRxEnvironments.update!(env::BayesianThermostat, elapsed_time)= add_temperature!(env, -0.1 * elapsed_time)","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we've fully specified our environment, and we can interact with it. In order to create the environment, we use the RxEnvironment struct, and we add an agent to this environment using add!:","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"environment = RxEnvironment(BayesianThermostat(0.0, -10, 10))\nagent = add!(environment, ThermostatAgent())","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we can have the agent conduct actions in our environment. Let's have the agent conduct some actions, and inspect the observations that are being returned by the environment:","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Rocket\n\n# Subscribe a logger actor to the observations of the agent\nRxEnvironments.subscribe_to_observations!(agent, logger())\n\n# Conduct 10 actions:\nfor i in 1:10\n    action = rand()\n    RxEnvironments.conduct_action!(agent, environment, action)\n    sleep(1/5)\nend","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"[LogActor] Data: 0.006170718477015863\n[LogActor] Data: -0.09624863445330185\n[LogActor] Data: -0.3269267933074502\n[LogActor] Data: 0.001304207094952492\n[LogActor] Data: 0.03626599314271475\n[LogActor] Data: 0.010733164205412482\n[LogActor] Data: 0.12313893922057219\n[LogActor] Data: -0.013042652548091921\n[LogActor] Data: 0.03561033321842316\n[LogActor] Data: 0.6763921880509323\n[LogActor] Data: 0.8313618838112217\n[LogActor] Data: 1.7408316683602163\n[LogActor] Data: 1.7322639115928715\n[LogActor] Data: 1.458556241545732\n[LogActor] Data: 1.6689296645689367\n[LogActor] Data: 1.683300152848493\n[LogActor] Data: 2.087509970813057\n[LogActor] Data: 2.258940017058188\n[LogActor] Data: 2.6537100822978372\n[LogActor] Data: 2.6012179767058408\n[LogActor] Data: 3.0775745739101716\n[LogActor] Data: 2.7326464283572727","category":"page"},{"location":"lib/getting_started/","page":"Getting Started","title":"Getting Started","text":"Congratulations! You've now implemented a basic environment in RxEnvironments.","category":"page"},{"location":"lib/philosophy/#lib-design-philosophy","page":"Design Philosophy","title":"Design Philosophy","text":"","category":"section"},{"location":"lib/philosophy/","page":"Design Philosophy","title":"Design Philosophy","text":"RxEnvironments is designed with Active Inference in mind. The philosophy behind Active Inference is based on a viewpoint that is somewhat esoteric if one is accustomed to traditional control or reinforcement learning terminology. This page aims to clarify the design principles of RxEnvironments. ","category":"page"},{"location":"lib/philosophy/#Agent-Environment-interaction","page":"Design Philosophy","title":"Agent-Environment interaction","text":"","category":"section"},{"location":"lib/philosophy/","page":"Design Philosophy","title":"Design Philosophy","text":"Classical reinforcement learning literature often makes the explicit distinction between an agent and the environment in which it lives. The interaction between agents and environments flows through the Markov Blanket of the agent; the agent emits actions and receives sensory observations and rewards. Similarly, the environment receives actions and emits observations and rewards to the agent. This symmetry of the Markov Blanket of both the agent and environment is crucial to notice: The actions of the agent are the observations of the environment, and vice versa. Consequently, one can argue that the environment is also an agent, which communicates to the agent through its Markov Blanket.","category":"page"},{"location":"lib/philosophy/#Entities-and-Markov-Blankets","page":"Design Philosophy","title":"Entities and Markov Blankets","text":"","category":"section"},{"location":"lib/philosophy/","page":"Design Philosophy","title":"Design Philosophy","text":"The observation in the previous paragraph calls for an overarching term for both agents and environments. RxEnvironments realizes this with the AbstractEntity type. An entity is anything that has a Markov Blanket and can therefore communicate with other entities. Both agents and environments are AbstractEntity's under the hood. The difference is that an environment should implement the update! function to encode the state transition, whereas in agents this should be replaced by an inference process that selects an action to perform. We do not explicitly funnel rewards back from environments to agents, instead, agents should interpret the observations it receives and attach value to them instead.","category":"page"},{"location":"lib/philosophy/#The-power-of-Markov-Blankets","page":"Design Philosophy","title":"The power of Markov Blankets","text":"","category":"section"},{"location":"lib/philosophy/","page":"Design Philosophy","title":"Design Philosophy","text":"All logic of RxEnvironments is written on the Entity level, this means that all logic holds for both agents and environments. For example: The fact that we support multi-agent environments also implies that an agent could emit actions to multiple environments as well, or that an agent can communicate with other agents in the same environment through its Markov Blanket. The concept of an Entity is what makes RxEnvironments so powerful and versatile. ","category":"page"},{"location":"#RxEnvironments.jl-Documentation","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"","category":"section"},{"location":"","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"Welcome to the documentation for RxEnvironments.jl! This document provides an overview of the package's functionality and usage.","category":"page"},{"location":"#Installation","page":"RxEnvironments.jl Documentation","title":"Installation","text":"","category":"section"},{"location":"","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"To install RxEnvironments.jl, use the following Julia command:","category":"page"},{"location":"","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"using Pkg\nPkg.add(\"RxEnvironments\")","category":"page"},{"location":"#Table-of-Contents","page":"RxEnvironments.jl Documentation","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"Pages = [\n  \"lib/getting_started.md\",\n]\nDepth = 2","category":"page"},{"location":"#Index","page":"RxEnvironments.jl Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"RxEnvironments.jl Documentation","title":"RxEnvironments.jl Documentation","text":"","category":"page"}]
}
